#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›ªçƒAPIçˆ¬è™«ï¼ˆæ¨èï¼‰
ä½¿ç”¨é›ªçƒå…¬å¼€APIè·å–ç”¨æˆ·å‘æ–‡ï¼Œæ¯”Seleniumæ›´ç¨³å®šå¿«é€Ÿ
"""

import sys
import json
import time
import argparse
import os
from datetime import datetime

try:
    import requests
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…requests: pip install requests", file=sys.stderr)
    sys.exit(1)


# æ®µæ°¸å¹³çš„é›ªçƒID
DEFAULT_USER_ID = "9528875558"


class XueqiuAPI:
    """é›ªçƒAPIå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://xueqiu.com',
        })
        self.base_url = "https://xueqiu.com"
    
    def init_cookies(self):
        """åˆå§‹åŒ–cookies - è®¿é—®é¦–é¡µè·å–å¿…è¦çš„cookies"""
        try:
            print("ğŸ” åˆå§‹åŒ–cookies...", file=sys.stderr)
            response = self.session.get(self.base_url)
            if response.status_code == 200:
                print("âœ… Cookiesåˆå§‹åŒ–æˆåŠŸ", file=sys.stderr)
                return True
            else:
                print(f"âš ï¸  åˆå§‹åŒ–cookieså¤±è´¥: {response.status_code}", file=sys.stderr)
                return False
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}", file=sys.stderr)
            return False
    
    def get_user_info(self, user_id):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        try:
            url = f"https://stock.xueqiu.com/v5/stock/portfolio/stock/list.json"
            params = {
                'user_id': user_id,
                'size': 1
            }
            response = self.session.get(url, params=params)
            
            if response.status_code == 200:
                data = response.json()
                return data
            else:
                print(f"âš ï¸  è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {response.status_code}", file=sys.stderr)
                return None
        
        except Exception as e:
            print(f"âŒ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}", file=sys.stderr)
            return None
    
    def get_user_posts(self, user_id, max_posts=20):
        """
        è·å–ç”¨æˆ·å‘æ–‡åˆ—è¡¨
        
        Args:
            user_id: é›ªçƒç”¨æˆ·ID
            max_posts: æœ€å¤šè·å–çš„å‘æ–‡æ•°
        
        Returns:
            list: å‘æ–‡åˆ—è¡¨
        """
        posts = []
        page = 1
        page_size = 20  # æ¯é¡µ20æ¡
        
        try:
            # è®¿é—®ç”¨æˆ·ä¸»é¡µè·å–ç”¨æˆ·å
            print(f"ğŸ“– æ­£åœ¨è®¿é—®ç”¨æˆ·ä¸»é¡µ...", file=sys.stderr)
            user_page_url = f"{self.base_url}/u/{user_id}"
            user_response = self.session.get(user_page_url)
            
            # å°è¯•ä»é¡µé¢æå–ç”¨æˆ·åï¼ˆå¯é€‰ï¼‰
            username = f"User_{user_id}"
            
            print(f"ğŸ‘¤ ç”¨æˆ·ID: {user_id}", file=sys.stderr)
            print(f"ğŸ“ å¼€å§‹è·å–å‘æ–‡...", file=sys.stderr)
            
            while len(posts) < max_posts:
                # é›ªçƒç”¨æˆ·åŠ¨æ€API
                api_url = f"https://xueqiu.com/statuses/original/timeline.json"
                params = {
                    'user_id': user_id,
                    'page': page,
                    'type': 0,  # 0=å…¨éƒ¨, 2=åŸåˆ›
                }
                
                print(f"ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page} é¡µ...", file=sys.stderr)
                response = self.session.get(api_url, params=params)
                
                if response.status_code != 200:
                    print(f"âš ï¸  APIè¯·æ±‚å¤±è´¥: {response.status_code}", file=sys.stderr)
                    # æ‰“å°å“åº”å†…å®¹ä¾›è°ƒè¯•
                    print(f"å“åº”: {response.text[:200]}", file=sys.stderr)
                    break
                
                try:
                    data = response.json()
                except json.JSONDecodeError:
                    print(f"âŒ JSONè§£æå¤±è´¥", file=sys.stderr)
                    break
                
                # æ£€æŸ¥è¿”å›çš„æ•°æ®ç»“æ„
                if 'list' not in data:
                    print(f"âš ï¸  è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {list(data.keys())}", file=sys.stderr)
                    break
                
                page_posts = data.get('list', [])
                
                if not page_posts:
                    print(f"ğŸ“­ æ²¡æœ‰æ›´å¤šå‘æ–‡äº†", file=sys.stderr)
                    break
                
                for post in page_posts:
                    if len(posts) >= max_posts:
                        break
                    
                    # æå–å‘æ–‡æ•°æ®
                    post_data = self._extract_post_data(post, user_id)
                    posts.append(post_data)
                    
                    title = post_data.get('title', '')[:50]
                    print(f"âœ… [{len(posts)}/{max_posts}] {title}...", file=sys.stderr)
                
                page += 1
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            print(f"\nâœ… å…±è·å– {len(posts)} æ¡å‘æ–‡", file=sys.stderr)
            return posts
        
        except Exception as e:
            print(f"âŒ è·å–å‘æ–‡å¤±è´¥: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            return posts
    
    def _extract_post_data(self, post, user_id):
        """æå–å‘æ–‡æ•°æ®"""
        post_data = {
            'id': str(post.get('id', '')),
            'user_id': user_id,
            'username': post.get('user', {}).get('screen_name', ''),
            'user_description': post.get('user', {}).get('description', ''),
            'title': post.get('title', ''),
            'text': post.get('text', ''),
            'description': post.get('description', ''),
            'created_at': post.get('created_at'),
            'source': post.get('source', ''),
            'like_count': post.get('like_count', 0),
            'reply_count': post.get('reply_count', 0),
            'retweet_count': post.get('retweet_count', 0),
            'fav_count': post.get('fav_count', 0),
            'view_count': post.get('view_count', 0),
            'url': f"https://xueqiu.com/{user_id}/{post.get('id', '')}",
            'scraped_at': datetime.now().isoformat()
        }
        
        # æ ¼å¼åŒ–æ—¶é—´
        if post_data['created_at']:
            try:
                timestamp = post_data['created_at'] / 1000  # æ¯«ç§’è½¬ç§’
                dt = datetime.fromtimestamp(timestamp)
                post_data['published_at'] = dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                post_data['published_at'] = ''
        
        return post_data


def save_to_file(posts, output_file):
    """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ JSONå·²ä¿å­˜åˆ°: {output_file}", file=sys.stderr)
        return True
    
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
        return False


def format_to_markdown(posts, user_id=""):
    """å°†å‘æ–‡æ ¼å¼åŒ–ä¸ºMarkdown"""
    username = posts[0].get('username', f'User_{user_id}') if posts else ''
    
    lines = [
        f"# ğŸ“Š {username} çš„é›ªçƒå‘æ–‡\n",
        f"ğŸ“… æŠ“å–æ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n",
        f"ğŸ“ å‘æ–‡æ•°é‡ï¼š{len(posts)}æ¡\n",
        f"ğŸ”— ç”¨æˆ·ä¸»é¡µï¼šhttps://xueqiu.com/u/{user_id}\n",
        "---\n",
    ]
    
    for i, post in enumerate(posts, 1):
        # æ ‡é¢˜
        title = post.get('title') or post.get('text', '')[:50] or 'æ— æ ‡é¢˜'
        lines.append(f"\n## {i}. {title}\n")
        
        # å‘å¸ƒæ—¶é—´å’Œæ¥æº
        pub_time = post.get('published_at', '')
        source = post.get('source', '')
        if pub_time:
            time_str = f"ğŸ•’ **å‘å¸ƒæ—¶é—´**: {pub_time}"
            if source:
                time_str += f" | æ¥è‡ª: {source}"
            lines.append(f"{time_str}\n")
        
        # é“¾æ¥
        if post.get('url'):
            lines.append(f"ğŸ”— **é“¾æ¥**: {post['url']}\n")
        
        # æ­£æ–‡å†…å®¹
        text = post.get('text') or post.get('description', '')
        if text:
            # æ¸…ç†HTMLæ ‡ç­¾ï¼ˆç®€å•å¤„ç†ï¼‰
            import re
            clean_text = re.sub(r'<[^>]+>', '', text)
            clean_text = clean_text.replace('&nbsp;', ' ').replace('&quot;', '"')
            lines.append(f"\n{clean_text}\n")
        
        # äº’åŠ¨æ•°æ®
        stats = []
        if post.get('like_count'):
            stats.append(f"ğŸ‘ {post['like_count']} èµ")
        if post.get('reply_count'):
            stats.append(f"ğŸ’¬ {post['reply_count']} è¯„è®º")
        if post.get('retweet_count'):
            stats.append(f"ğŸ”„ {post['retweet_count']} è½¬å‘")
        if post.get('view_count'):
            stats.append(f"ğŸ‘ï¸ {post['view_count']} é˜…è¯»")
        
        if stats:
            lines.append(f"\n*{' | '.join(stats)}*\n")
        
        lines.append("\n---\n")
    
    return '\n'.join(lines)


def main():
    parser = argparse.ArgumentParser(description='é›ªçƒAPIçˆ¬è™«ï¼ˆæ¨èä½¿ç”¨ï¼‰')
    parser.add_argument('--user-id', type=str, default=DEFAULT_USER_ID,
                        help=f'é›ªçƒç”¨æˆ·ID (é»˜è®¤: {DEFAULT_USER_ID} - æ®µæ°¸å¹³)')
    parser.add_argument('--max-posts', type=int, default=20,
                        help='æœ€å¤šè·å–çš„å‘æ–‡æ•°é‡ (é»˜è®¤: 20)')
    parser.add_argument('--output', type=str, default='../output/xueqiu_posts.json',
                        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--format', type=str, choices=['json', 'markdown', 'both'], default='both',
                        help='è¾“å‡ºæ ¼å¼ (é»˜è®¤: both)')
    
    args = parser.parse_args()
    
    print("=" * 60, file=sys.stderr)
    print("ğŸ“Š é›ªçƒAPIçˆ¬è™«", file=sys.stderr)
    print("=" * 60, file=sys.stderr)
    print(f"ğŸ‘¤ ç”¨æˆ·ID: {args.user_id}", file=sys.stderr)
    print(f"ğŸ“ æœ€å¤šè·å–: {args.max_posts}æ¡", file=sys.stderr)
    print(f"ğŸ’¾ è¾“å‡ºæ ¼å¼: {args.format}", file=sys.stderr)
    print("=" * 60, file=sys.stderr)
    print("", file=sys.stderr)
    
    # åˆ›å»ºAPIå®¢æˆ·ç«¯
    api = XueqiuAPI()
    
    # åˆå§‹åŒ–cookies
    if not api.init_cookies():
        print("âš ï¸  Cookiesåˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­å°è¯•...", file=sys.stderr)
    
    time.sleep(1)
    
    # è·å–å‘æ–‡
    posts = api.get_user_posts(args.user_id, args.max_posts)
    
    if not posts:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•å‘æ–‡", file=sys.stderr)
        print("\nğŸ’¡ å¯èƒ½çš„åŸå› ï¼š", file=sys.stderr)
        print("   1. ç”¨æˆ·IDä¸æ­£ç¡®", file=sys.stderr)
        print("   2. ç”¨æˆ·æ²¡æœ‰å…¬å¼€å‘æ–‡", file=sys.stderr)
        print("   3. APIè®¿é—®å—é™ï¼ˆéœ€è¦ç™»å½•ï¼‰", file=sys.stderr)
        print(f"\nè¯·è®¿é—® https://xueqiu.com/u/{args.user_id} ç¡®è®¤ç”¨æˆ·ID", file=sys.stderr)
        return
    
    # ä¿å­˜æ•°æ®
    if args.format in ['json', 'both']:
        save_to_file(posts, args.output)
    
    if args.format in ['markdown', 'both']:
        markdown_file = args.output.replace('.json', '.md')
        markdown_content = format_to_markdown(posts, args.user_id)
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"ğŸ“ Markdownå·²ä¿å­˜åˆ°: {markdown_file}", file=sys.stderr)
    
    # è¾“å‡ºåˆ°stdout
    if args.format == 'markdown':
        print(format_to_markdown(posts, args.user_id))
    else:
        print(json.dumps(posts, ensure_ascii=False, indent=2))
    
    print("\nâœ… æŠ“å–å®Œæˆï¼", file=sys.stderr)


if __name__ == '__main__':
    main()

