#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœæ—¥æ–°é—»RSSæŠ“å–è„šæœ¬
è·å–æœæ—¥æ–°é—»RSS feedå¹¶ä¿å­˜ä¸ºJSONæ ¼å¼
æ”¯æŒå¤šä¸ªåˆ†ç±»çš„RSSæº
"""

import sys
import json
import os
from datetime import datetime

try:
    import feedparser
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…feedparser: pip install feedparser", file=sys.stderr)
    sys.exit(1)

try:
    import requests
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…requests: pip install requests", file=sys.stderr)
    sys.exit(1)


# RSSé“¾æ¥é…ç½® - æ‰€æœ‰åˆ†ç±»
RSS_SOURCES = {
    "newsheadlines": {
        "url": "https://www.asahi.com/rss/asahi/newsheadlines.rdf",
        "name": "ç»¼åˆå¤´æ¡",
        "emoji": "ğŸ“°"
    },
    "national": {
        "url": "https://www.asahi.com/rss/asahi/national.rdf",
        "name": "ç¤¾ä¼šæ–°é—»",
        "emoji": "ğŸ˜ï¸"
    },
    "international": {
        "url": "https://www.asahi.com/rss/asahi/international.rdf",
        "name": "å›½é™…æ–°é—»",
        "emoji": "ğŸŒ"
    },
    "politics": {
        "url": "https://www.asahi.com/rss/asahi/politics.rdf",
        "name": "æ”¿æ²»æ–°é—»",
        "emoji": "ğŸ›ï¸"
    },
    "business": {
        "url": "https://www.asahi.com/rss/asahi/business.rdf",
        "name": "ç»æµæ–°é—»",
        "emoji": "ğŸ’¼"
    },
    "sports": {
        "url": "https://www.asahi.com/rss/asahi/sports.rdf",
        "name": "ä½“è‚²æ–°é—»",
        "emoji": "âš½"
    },
    "culture": {
        "url": "https://www.asahi.com/rss/asahi/culture.rdf",
        "name": "æ–‡åŒ–æ–°é—»",
        "emoji": "ğŸ­"
    },
    "science": {
        "url": "https://www.asahi.com/rss/asahi/science.rdf",
        "name": "ç§‘å­¦æ–°é—»",
        "emoji": "ğŸ”¬"
    }
}

# è¾“å‡ºç›®å½•
OUTPUT_DIR = os.path.expanduser("~/Desktop/workspace/brain/skynet")


def fetch_rss_feed(url):
    """
    è·å–RSS feed
    
    Args:
        url: RSS feedçš„URL
    
    Returns:
        feedparserè§£æåçš„å¯¹è±¡
    """
    try:
        print(f"ğŸ“¡ æ­£åœ¨è·å–RSS: {url}", file=sys.stderr)
        
        # ä½¿ç”¨requestsè·å–å†…å®¹ï¼ˆå¯ä»¥è®¾ç½®è¶…æ—¶å’Œheadersï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # ä½¿ç”¨feedparserè§£æ
        feed = feedparser.parse(response.content)
        
        print(f"âœ… æˆåŠŸè·å– {len(feed.entries)} æ¡æ–°é—»", file=sys.stderr)
        return feed
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è·å–RSSå¤±è´¥: {e}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"âŒ è§£æRSSå¤±è´¥: {e}", file=sys.stderr)
        return None


def parse_date(entry):
    """
    è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºæ ‡å‡†æ ¼å¼
    
    Args:
        entry: RSS entryå¯¹è±¡
    
    Returns:
        æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD HH:MM:SS)
    """
    try:
        # å°è¯•å¤šä¸ªæ—¥æœŸå­—æ®µ
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            dt = datetime(*entry.published_parsed[:6])
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            dt = datetime(*entry.updated_parsed[:6])
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        elif 'published' in entry:
            # å°è¯•ç›´æ¥è§£æå­—ç¬¦ä¸²
            return entry.published
        return ""
    except Exception as e:
        return ""


def convert_to_json_format(feed, category_key="", category_name="", start_id=1):
    """
    å°†RSS feedè½¬æ¢ä¸ºæŒ‡å®šçš„JSONæ ¼å¼
    
    Args:
        feed: feedparserè§£æåçš„feedå¯¹è±¡
        category_key: åˆ†ç±»é”®åï¼ˆå¦‚ "national"ï¼‰
        category_name: åˆ†ç±»åç§°ï¼ˆå¦‚ "ç¤¾ä¼šæ–°é—»"ï¼‰
        start_id: èµ·å§‹ID
    
    Returns:
        list: JSONæ ¼å¼çš„æ–°é—»åˆ—è¡¨
    """
    news_list = []
    
    for idx, entry in enumerate(feed.entries, start=start_id):
        news_item = {
            "id": idx,
            "category": category_key,
            "category_name": category_name,
            "title": entry.get('title', ''),
            "link": entry.get('link', ''),
            "pubDate": parse_date(entry),
            "content": entry.get('description', ''),
            "contentSnippet": entry.get('summary', '')
        }
        news_list.append(news_item)
    
    return news_list


def save_to_json(news_list, output_dir, category_stats=None):
    """
    ä¿å­˜æ–°é—»åˆ—è¡¨ä¸ºJSONæ–‡ä»¶
    
    Args:
        news_list: æ–°é—»åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        category_stats: åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆå½“å‰æ—¥æœŸï¼‰
        today = datetime.now()
        filename = f"asahi_all_news_{today.strftime('%Y%m%d')}.json"
        filepath = os.path.join(output_dir, filename)
        
        # ä¿å­˜JSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}", file=sys.stderr)
        print(f"ğŸ“Š å…±ä¿å­˜ {len(news_list)} æ¡æ–°é—»", file=sys.stderr)
        
        # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡
        if category_stats:
            print("\nğŸ“‹ åˆ†ç±»ç»Ÿè®¡:", file=sys.stderr)
            for cat_key, info in category_stats.items():
                print(f"   {info['emoji']} {info['name']}: {info['count']} æ¡", file=sys.stderr)
        
        return filepath
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
        return None


def print_summary(news_list, category_stats):
    """æ‰“å°æ–°é—»æ‘˜è¦"""
    print("\n" + "="*60, file=sys.stderr)
    print("ğŸ“° æœæ—¥æ–°é—» - å…¨åˆ†ç±»æ–°é—»æ±‡æ€»", file=sys.stderr)
    print("="*60, file=sys.stderr)
    
    # æŒ‰åˆ†ç±»æ˜¾ç¤ºæ–°é—»
    for cat_key, info in category_stats.items():
        if info['count'] == 0:
            continue
            
        print(f"\n{info['emoji']} {info['name']} ({info['count']} æ¡)", file=sys.stderr)
        print("-" * 60, file=sys.stderr)
        
        # è·å–è¯¥åˆ†ç±»çš„å‰3æ¡æ–°é—»
        cat_news = [n for n in news_list if n['category'] == cat_key][:3]
        for news in cat_news:
            print(f"  â€¢ {news['title']}", file=sys.stderr)
            print(f"    {news['pubDate']}", file=sys.stderr)
    
    print("\n" + "="*60, file=sys.stderr)


def main():
    """ä¸»å‡½æ•°"""
    print("="*60, file=sys.stderr)
    print("ğŸ“° æœæ—¥æ–°é—»RSSæŠ“å–å·¥å…· - å…¨åˆ†ç±»ç‰ˆ", file=sys.stderr)
    print("="*60, file=sys.stderr)
    print("", file=sys.stderr)
    
    all_news = []
    category_stats = {}
    
    # å¾ªç¯æŠ“å–æ‰€æœ‰RSSæº
    for cat_key, cat_info in RSS_SOURCES.items():
        print(f"\n{cat_info['emoji']} æ­£åœ¨æŠ“å–: {cat_info['name']}", file=sys.stderr)
        
        # è·å–RSS feed
        feed = fetch_rss_feed(cat_info['url'])
        
        if not feed or not feed.entries:
            print(f"âš ï¸  {cat_info['name']} æœªèƒ½è·å–æ•°æ®ï¼Œè·³è¿‡", file=sys.stderr)
            category_stats[cat_key] = {
                'name': cat_info['name'],
                'emoji': cat_info['emoji'],
                'count': 0
            }
            continue
        
        # è½¬æ¢ä¸ºJSONæ ¼å¼ï¼ˆä¸´æ—¶IDï¼Œç¨åç»Ÿä¸€ç¼–å·ï¼‰
        news_list = convert_to_json_format(
            feed, 
            category_key=cat_key,
            category_name=cat_info['name'],
            start_id=0
        )
        
        # æ·»åŠ åˆ°æ€»åˆ—è¡¨
        all_news.extend(news_list)
        
        # ç»Ÿè®¡ä¿¡æ¯
        category_stats[cat_key] = {
            'name': cat_info['name'],
            'emoji': cat_info['emoji'],
            'count': len(news_list)
        }
    
    if not all_news:
        print("\nâŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®", file=sys.stderr)
        sys.exit(1)
    
    # ç»Ÿä¸€é‡æ–°ç¼–å·
    for idx, news in enumerate(all_news, start=1):
        news['id'] = idx
    
    print(f"\nğŸ“Š æ€»è®¡è·å– {len(all_news)} æ¡æ–°é—»", file=sys.stderr)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    filepath = save_to_json(all_news, OUTPUT_DIR, category_stats)
    if not filepath:
        sys.exit(1)
    
    # æ‰“å°æ‘˜è¦
    print_summary(all_news, category_stats)
    
    # è¾“å‡ºJSONåˆ°stdoutï¼ˆå¯é€‰ï¼‰
    print(json.dumps(all_news, ensure_ascii=False, indent=2))
    
    print("\nâœ… æŠ“å–å®Œæˆï¼", file=sys.stderr)


if __name__ == '__main__':
    main()

