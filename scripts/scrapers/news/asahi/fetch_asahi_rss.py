#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœæ—¥æ–°é—»RSSæŠ“å–è„šæœ¬
è·å–æœæ—¥æ–°é—»RSS feedå¹¶ä¿å­˜ä¸ºJSONæ ¼å¼
"""

import sys
import json
import os
from datetime import datetime

try:
    import feedparser
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…feedparser: pip install feedparser", file=sys.stderr)
    sys.exit(1)

try:
    import requests
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…requests: pip install requests", file=sys.stderr)
    sys.exit(1)


# RSSé“¾æ¥
RSS_URL = "https://www.asahi.com/rss/asahi/newsheadlines.rdf"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "/Users/a0000/Desktop/workspace/brain/skynet"


def fetch_rss_feed(url):
    """
    è·å–RSS feed
    
    Args:
        url: RSS feedçš„URL
    
    Returns:
        feedparserè§£æåçš„å¯¹è±¡
    """
    try:
        print(f"ğŸ“¡ æ­£åœ¨è·å–RSS: {url}", file=sys.stderr)
        
        # ä½¿ç”¨requestsè·å–å†…å®¹ï¼ˆå¯ä»¥è®¾ç½®è¶…æ—¶å’Œheadersï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # ä½¿ç”¨feedparserè§£æ
        feed = feedparser.parse(response.content)
        
        print(f"âœ… æˆåŠŸè·å– {len(feed.entries)} æ¡æ–°é—»", file=sys.stderr)
        return feed
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è·å–RSSå¤±è´¥: {e}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"âŒ è§£æRSSå¤±è´¥: {e}", file=sys.stderr)
        return None


def parse_date(entry):
    """
    è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºæ ‡å‡†æ ¼å¼
    
    Args:
        entry: RSS entryå¯¹è±¡
    
    Returns:
        æ ¼å¼åŒ–çš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD HH:MM:SS)
    """
    try:
        # å°è¯•å¤šä¸ªæ—¥æœŸå­—æ®µ
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            dt = datetime(*entry.published_parsed[:6])
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            dt = datetime(*entry.updated_parsed[:6])
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        elif 'published' in entry:
            # å°è¯•ç›´æ¥è§£æå­—ç¬¦ä¸²
            return entry.published
        return ""
    except Exception as e:
        return ""


def convert_to_json_format(feed, start_id=1):
    """
    å°†RSS feedè½¬æ¢ä¸ºæŒ‡å®šçš„JSONæ ¼å¼
    
    Args:
        feed: feedparserè§£æåçš„feedå¯¹è±¡
        start_id: èµ·å§‹ID
    
    Returns:
        list: JSONæ ¼å¼çš„æ–°é—»åˆ—è¡¨
    """
    news_list = []
    
    for idx, entry in enumerate(feed.entries, start=start_id):
        news_item = {
            "id": idx,
            "title": entry.get('title', ''),
            "link": entry.get('link', ''),
            "pubDate": parse_date(entry),
            "content": entry.get('description', ''),
            "contentSnippet": entry.get('summary', '')
        }
        news_list.append(news_item)
    
    return news_list


def save_to_json(news_list, output_dir):
    """
    ä¿å­˜æ–°é—»åˆ—è¡¨ä¸ºJSONæ–‡ä»¶
    
    Args:
        news_list: æ–°é—»åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆå½“å‰æ—¥æœŸï¼‰
        today = datetime.now()
        filename = f"asahi_newsheadlines_{today.strftime('%Y%m%d')}.json"
        filepath = os.path.join(output_dir, filename)
        
        # ä¿å­˜JSON
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}", file=sys.stderr)
        print(f"ğŸ“Š å…±ä¿å­˜ {len(news_list)} æ¡æ–°é—»", file=sys.stderr)
        
        return filepath
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)
        return None


def print_summary(news_list):
    """æ‰“å°æ–°é—»æ‘˜è¦"""
    print("\n" + "="*60, file=sys.stderr)
    print("ğŸ“° æœæ—¥æ–°é—» - ä»Šæ—¥å¤´æ¡", file=sys.stderr)
    print("="*60, file=sys.stderr)
    
    for news in news_list[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
        print(f"\n[{news['id']}] {news['title']}", file=sys.stderr)
        print(f"    ğŸ”— {news['link']}", file=sys.stderr)
        print(f"    ğŸ•’ {news['pubDate']}", file=sys.stderr)
    
    if len(news_list) > 5:
        print(f"\n... è¿˜æœ‰ {len(news_list) - 5} æ¡æ–°é—»", file=sys.stderr)
    
    print("\n" + "="*60, file=sys.stderr)


def main():
    """ä¸»å‡½æ•°"""
    print("="*60, file=sys.stderr)
    print("ğŸ“° æœæ—¥æ–°é—»RSSæŠ“å–å·¥å…·", file=sys.stderr)
    print("="*60, file=sys.stderr)
    print("", file=sys.stderr)
    
    # è·å–RSS feed
    feed = fetch_rss_feed(RSS_URL)
    if not feed or not feed.entries:
        print("âŒ æœªèƒ½è·å–åˆ°æ–°é—»æ•°æ®", file=sys.stderr)
        sys.exit(1)
    
    # è½¬æ¢ä¸ºJSONæ ¼å¼
    news_list = convert_to_json_format(feed)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    filepath = save_to_json(news_list, OUTPUT_DIR)
    if not filepath:
        sys.exit(1)
    
    # æ‰“å°æ‘˜è¦
    print_summary(news_list)
    
    # è¾“å‡ºJSONåˆ°stdoutï¼ˆå¯é€‰ï¼‰
    print(json.dumps(news_list, ensure_ascii=False, indent=2))
    
    print("\nâœ… æŠ“å–å®Œæˆï¼", file=sys.stderr)


if __name__ == '__main__':
    main()

