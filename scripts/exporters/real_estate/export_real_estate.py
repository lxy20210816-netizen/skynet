#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¼å‡ºæˆ¿åœ°äº§æ± æ•°æ®åˆ°Markdownæ ¼å¼
ä»Google Sheetsè¯»å–æ•°æ®ï¼Œç¾åŒ–è¾“å‡ºåˆ°æŒ‡å®šç›®å½•
"""

import os
import sys
import json
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
from collections import defaultdict

# Google Sheets APIé…ç½®
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
CREDENTIALS_FILE = os.path.expanduser('~/Desktop/workspace/skynet/config/credentials.json')
SHEET_ID = '1b55-D54NLbBo1yJd-OjDy7rqCBEkK8Dza3vDLZCPaiU'
WORKSHEET_NAME = 'æˆ¿åœ°äº§æ± '

# è¾“å‡ºç›®å½•
OUTPUT_DIR = os.path.expanduser('~/Desktop/workspace/brain/ä¸åŠ¨äº§æ± ')


def read_from_google_sheets():
    """ä»Google Sheetsè¯»å–æ•°æ®"""
    try:
        print("ğŸ” æ­£åœ¨è®¤è¯Google Sheets...", file=sys.stderr)
        creds = Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=SCOPES)
        client = gspread.authorize(creds)
        
        print(f"ğŸ“– æ­£åœ¨æ‰“å¼€è¡¨æ ¼...", file=sys.stderr)
        spreadsheet = client.open_by_key(SHEET_ID)
        worksheet = spreadsheet.worksheet(WORKSHEET_NAME)
        
        print(f"ğŸ“¥ æ­£åœ¨è¯»å–æ•°æ®...", file=sys.stderr)
        all_values = worksheet.get_all_values()
        
        if not all_values:
            print("âŒ å·¥ä½œè¡¨ä¸ºç©º", file=sys.stderr)
            return []
        
        headers = all_values[0]
        data = []
        
        for row in all_values[1:]:
            if not any(row):  # è·³è¿‡ç©ºè¡Œ
                continue
            row_dict = {}
            for i, header in enumerate(headers):
                if i < len(row):
                    row_dict[header] = row[i]
                else:
                    row_dict[header] = ''
            data.append(row_dict)
        
        print(f"âœ… æˆåŠŸè¯»å– {len(data)} æ¡æ•°æ®", file=sys.stderr)
        return data
    
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°å‡­è¯æ–‡ä»¶: {CREDENTIALS_FILE}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)


def parse_price(price_str):
    """è§£æä»·æ ¼å­—ç¬¦ä¸²ï¼Œè¿”å›æ•°å€¼"""
    if not price_str:
        return 0
    try:
        # ç§»é™¤é€—å·ï¼Œåªä¿ç•™æ•°å­—
        price_str = price_str.replace(',', '').strip()
        return float(price_str) if price_str else 0
    except:
        return 0


def parse_area(area_str):
    """è§£æé¢ç§¯å­—ç¬¦ä¸²ï¼Œè¿”å›æ•°å€¼"""
    if not area_str:
        return 0
    try:
        # æå–ç¬¬ä¸€ä¸ªæ•°å­—
        import re
        match = re.search(r'([\d.]+)', area_str)
        return float(match.group(1)) if match else 0
    except:
        return 0


def format_property(prop):
    """æ ¼å¼åŒ–å•ä¸ªæˆ¿äº§ä¿¡æ¯ä¸ºMarkdown"""
    lines = []
    
    # æ ‡é¢˜
    building_name = prop.get('ğŸ¢ ç‰©ä»¶åç§°', 'N/A')
    lines.append(f"### {building_name}\n")
    
    # åŸºæœ¬ä¿¡æ¯è¡¨æ ¼
    lines.append("| é¡¹ç›® | ä¿¡æ¯ |")
    lines.append("|------|------|")
    
    # ä»·æ ¼
    price = prop.get('ğŸ’° ä»·æ ¼(ä¸‡å††)', '')
    price_per_sqm = prop.get('ğŸ“Š å•ä»·(ä¸‡å††/mÂ²)', '')
    if price:
        lines.append(f"| ğŸ’° ä»·æ ¼ | **{price}ä¸‡å††** |")
    if price_per_sqm:
        lines.append(f"| ğŸ“Š å•ä»· | {price_per_sqm} ä¸‡å††/mÂ² |")
    
    # é¢ç§¯å’Œæˆ·å‹
    area = prop.get('ğŸ“ é¢ç§¯(mÂ²)', '')
    layout = prop.get('ğŸ  æˆ·å‹', '')
    if area:
        lines.append(f"| ğŸ“ é¢ç§¯ | {area}mÂ² |")
    if layout:
        lines.append(f"| ğŸ  æˆ·å‹ | {layout} |")
    
    # å»ºç­‘å¹´ä»½å’Œæˆ¿é¾„
    year = prop.get('ğŸ“… å»ºé€ å¹´ä»½', '')
    age = prop.get('â³ æˆ¿é¾„(å¹´)', '')
    if year:
        lines.append(f"| ğŸ“… å»ºé€ å¹´ä»½ | {year}å¹´ |")
    if age:
        lines.append(f"| â³ æˆ¿é¾„ | {age}å¹´ |")
    
    # åœ°å€
    address = prop.get('ğŸ“ åœ°å€', '')
    if address:
        lines.append(f"| ğŸ“ åœ°å€ | {address} |")
    
    # äº¤é€š
    access = prop.get('ğŸš‡ äº¤é€š', '')
    if access and access != 'N/A':
        lines.append(f"| ğŸš‡ äº¤é€š | {access} |")
    
    # é“¾æ¥
    url = prop.get('ğŸ”— è¯¦æƒ…é“¾æ¥', '')
    if url and url != 'N/A':
        lines.append(f"| ğŸ”— è¯¦æƒ… | [æŸ¥çœ‹è¯¦æƒ…]({url}) |")
    
    lines.append("")  # ç©ºè¡Œ
    return '\n'.join(lines)


def generate_markdown(data):
    """ç”ŸæˆMarkdownæ–‡æ¡£"""
    # æŒ‰åœ°åŒºå’Œç±»å‹åˆ†ç»„
    grouped = defaultdict(lambda: defaultdict(list))
    
    for prop in data:
        region = prop.get('ğŸ—ºï¸ åœ°åŒº', 'æœªçŸ¥')
        prop_type = prop.get('ğŸ˜ï¸ ç±»å‹', 'æœªçŸ¥')
        grouped[region][prop_type].append(prop)
    
    # ç”Ÿæˆæ€»è§ˆæ–‡æ¡£
    overview_lines = [
        "# ğŸ˜ï¸ ä¸åŠ¨äº§æ± å­æ€»è§ˆ\n",
        f"ğŸ“… æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n",
        "---\n",
    ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(data)
    overview_lines.append(f"ğŸ“Š **æ€»æˆ¿æºæ•°é‡**: {total_count} ä¸ª\n")
    
    # æŒ‰åœ°åŒºç»Ÿè®¡
    overview_lines.append("## ğŸ“ åœ°åŒºåˆ†å¸ƒ\n")
    for region in sorted(grouped.keys()):
        region_count = sum(len(props) for props in grouped[region].values())
        overview_lines.append(f"- **{region}**: {region_count} ä¸ªæˆ¿æº")
        for prop_type in sorted(grouped[region].keys()):
            count = len(grouped[region][prop_type])
            overview_lines.append(f"  - {prop_type}: {count} ä¸ª")
    overview_lines.append("")
    
    # ä»·æ ¼åˆ†æ
    overview_lines.append("## ğŸ’° ä»·æ ¼åˆ†æ\n")
    
    prices = []
    for prop in data:
        price = parse_price(prop.get('ğŸ’° ä»·æ ¼(ä¸‡å††)', ''))
        if price > 0:
            prices.append(price)
    
    if prices:
        avg_price = sum(prices) / len(prices)
        min_price = min(prices)
        max_price = max(prices)
        
        overview_lines.append(f"- å¹³å‡ä»·æ ¼: **{avg_price:.0f}ä¸‡å††**")
        overview_lines.append(f"- æœ€ä½ä»·æ ¼: {min_price:.0f}ä¸‡å††")
        overview_lines.append(f"- æœ€é«˜ä»·æ ¼: {max_price:.0f}ä¸‡å††")
        overview_lines.append("")
    
    # é¢ç§¯åˆ†æ
    overview_lines.append("## ğŸ“ é¢ç§¯åˆ†æ\n")
    
    areas = []
    for prop in data:
        area = parse_area(prop.get('ğŸ“ é¢ç§¯(mÂ²)', ''))
        if area > 0:
            areas.append(area)
    
    if areas:
        avg_area = sum(areas) / len(areas)
        min_area = min(areas)
        max_area = max(areas)
        
        overview_lines.append(f"- å¹³å‡é¢ç§¯: **{avg_area:.2f}mÂ²**")
        overview_lines.append(f"- æœ€å°é¢ç§¯: {min_area:.2f}mÂ²")
        overview_lines.append(f"- æœ€å¤§é¢ç§¯: {max_area:.2f}mÂ²")
        overview_lines.append("")
    
    # åœ°åŒºé“¾æ¥å¯¼èˆª
    overview_lines.append("## ğŸ—‚ï¸ åˆ†åŒºè¯¦æƒ…\n")
    for region in sorted(grouped.keys()):
        safe_region = region.replace('/', '_')
        overview_lines.append(f"- [{region}](./{safe_region}/README.md)")
    overview_lines.append("")
    
    return '\n'.join(overview_lines), grouped


def generate_region_markdown(region, properties_by_type):
    """ç”Ÿæˆåœ°åŒºè¯¦æƒ…Markdown"""
    lines = [
        f"# ğŸ—ºï¸ {region}\n",
        f"ğŸ“… æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n",
        "---\n",
    ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    total = sum(len(props) for props in properties_by_type.values())
    lines.append(f"ğŸ“Š **æˆ¿æºæ•°é‡**: {total} ä¸ª\n")
    
    # æŒ‰ç±»å‹å¯¼èˆª
    lines.append("## ğŸ“‘ åˆ†ç±»\n")
    for prop_type in sorted(properties_by_type.keys()):
        count = len(properties_by_type[prop_type])
        safe_type = prop_type.replace('/', '_')
        lines.append(f"- [{prop_type}](./{safe_type}.md) ({count}ä¸ª)")
    lines.append("\n---\n")
    
    # è¿”å›é¦–é¡µé“¾æ¥
    lines.append("[â† è¿”å›æ€»è§ˆ](../README.md)\n")
    
    return '\n'.join(lines)


def generate_type_markdown(region, prop_type, properties):
    """ç”Ÿæˆç±»å‹è¯¦æƒ…Markdown"""
    lines = [
        f"# {region} - {prop_type}\n",
        f"ğŸ“… æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n",
        f"ğŸ“Š æˆ¿æºæ•°é‡ï¼š{len(properties)} ä¸ª\n",
        "---\n",
    ]
    
    # æŒ‰ä»·æ ¼æ’åº
    properties_sorted = sorted(properties, key=lambda x: parse_price(x.get('ğŸ’° ä»·æ ¼(ä¸‡å††)', '')))
    
    # è¾“å‡ºæ¯ä¸ªæˆ¿äº§
    for i, prop in enumerate(properties_sorted, 1):
        lines.append(f"## {i}. {prop.get('ğŸ¢ ç‰©ä»¶åç§°', 'N/A')}\n")
        lines.append(format_property(prop))
        lines.append("---\n")
    
    # è¿”å›é“¾æ¥
    safe_region = region.replace('/', '_')
    lines.append(f"[â† è¿”å›{region}](./{safe_region}/README.md) | [â† è¿”å›æ€»è§ˆ](../README.md)\n")
    
    return '\n'.join(lines)


def generate_unified_markdown(data):
    """ç”Ÿæˆç»Ÿä¸€çš„Markdownæ–‡æ¡£"""
    # æŒ‰åœ°åŒºå’Œç±»å‹åˆ†ç»„
    grouped = defaultdict(lambda: defaultdict(list))
    
    for prop in data:
        region = prop.get('ğŸ—ºï¸ åœ°åŒº', 'æœªçŸ¥')
        prop_type = prop.get('ğŸ˜ï¸ ç±»å‹', 'æœªçŸ¥')
        grouped[region][prop_type].append(prop)
    
    # ç”Ÿæˆæ–‡æ¡£
    lines = [
        "# ğŸ˜ï¸ ä¸åŠ¨äº§æ± \n",
        f"ğŸ“… æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n",
        "---\n",
    ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_count = len(data)
    lines.append(f"ğŸ“Š **æ€»æˆ¿æºæ•°é‡**: {total_count} ä¸ª\n")
    
    # ä»·æ ¼ç»Ÿè®¡
    prices = []
    for prop in data:
        price = parse_price(prop.get('ğŸ’° ä»·æ ¼(ä¸‡å††)', ''))
        if price > 0:
            prices.append(price)
    
    if prices:
        avg_price = sum(prices) / len(prices)
        min_price = min(prices)
        max_price = max(prices)
        lines.append(f"ğŸ’° **ä»·æ ¼åŒºé—´**: {min_price:.0f}ä¸‡å†† - {max_price:.0f}ä¸‡å††ï¼ˆå¹³å‡ {avg_price:.0f}ä¸‡å††ï¼‰\n")
    
    # é¢ç§¯ç»Ÿè®¡
    areas = []
    for prop in data:
        area = parse_area(prop.get('ğŸ“ é¢ç§¯(mÂ²)', ''))
        if area > 0:
            areas.append(area)
    
    if areas:
        avg_area = sum(areas) / len(areas)
        lines.append(f"ğŸ“ **é¢ç§¯åŒºé—´**: {min(areas):.1f}mÂ² - {max(areas):.1f}mÂ²ï¼ˆå¹³å‡ {avg_area:.1f}mÂ²ï¼‰\n")
    
    lines.append("\n---\n")
    
    # æŒ‰åœ°åŒºè¾“å‡º
    for region in sorted(grouped.keys()):
        lines.append(f"\n## ğŸ“ {region}\n")
        
        properties_by_type = grouped[region]
        total_region = sum(len(props) for props in properties_by_type.values())
        lines.append(f"**æˆ¿æºæ•°é‡**: {total_region} ä¸ª\n")
        
        # æŒ‰ç±»å‹è¾“å‡º
        for prop_type in sorted(properties_by_type.keys()):
            properties = properties_by_type[prop_type]
            lines.append(f"\n### {prop_type} ({len(properties)}ä¸ª)\n")
            
            # æŒ‰ä»·æ ¼æ’åº
            properties_sorted = sorted(properties, key=lambda x: parse_price(x.get('ğŸ’° ä»·æ ¼(ä¸‡å††)', '')))
            
            # è¾“å‡ºæ¯ä¸ªæˆ¿äº§
            for i, prop in enumerate(properties_sorted, 1):
                building_name = prop.get('ğŸ¢ ç‰©ä»¶åç§°', 'N/A')
                price = prop.get('ğŸ’° ä»·æ ¼(ä¸‡å††)', '')
                price_per_sqm = prop.get('ğŸ“Š å•ä»·(ä¸‡å††/mÂ²)', '')
                area = prop.get('ğŸ“ é¢ç§¯(mÂ²)', '')
                layout = prop.get('ğŸ  æˆ·å‹', '')
                year = prop.get('ğŸ“… å»ºé€ å¹´ä»½', '')
                age = prop.get('â³ æˆ¿é¾„(å¹´)', '')
                address = prop.get('ğŸ“ åœ°å€', '')
                url = prop.get('ğŸ”— è¯¦æƒ…é“¾æ¥', '')
                
                # ç®€æ´æ ¼å¼
                lines.append(f"\n**{i}. {building_name}**")
                
                info_parts = []
                if price:
                    info_parts.append(f"ğŸ’° {price}ä¸‡å††")
                if price_per_sqm:
                    info_parts.append(f"ğŸ“Š {price_per_sqm}ä¸‡å††/mÂ²")
                if area:
                    info_parts.append(f"ğŸ“ {area}mÂ²")
                if layout:
                    info_parts.append(f"ğŸ  {layout}")
                if year:
                    info_parts.append(f"ğŸ“… {year}å¹´")
                if age:
                    info_parts.append(f"â³ {age}å¹´")
                
                if info_parts:
                    lines.append(" | " + " | ".join(info_parts))
                
                if address:
                    lines.append(f"  \nğŸ“ {address}")
                
                if url and url != 'N/A':
                    lines.append(f"  \nğŸ”— [è¯¦æƒ…]({url})")
                
                lines.append("")  # ç©ºè¡Œ
            
            lines.append("")  # ç±»å‹ä¹‹é—´ç©ºè¡Œ
    
    return '\n'.join(lines)


def export_data(data):
    """å¯¼å‡ºæ•°æ®åˆ°Markdownæ–‡ä»¶"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}", file=sys.stderr)
        
        # ç”Ÿæˆç»Ÿä¸€çš„Markdownæ–‡æ¡£
        full_markdown = generate_unified_markdown(data)
        
        # å†™å…¥ä¸»æ–‡ä»¶
        main_path = os.path.join(OUTPUT_DIR, 'ä¸åŠ¨äº§æ± .md')
        with open(main_path, 'w', encoding='utf-8') as f:
            f.write(full_markdown)
        print(f"âœ… å·²ç”Ÿæˆä¸»æ–‡æ¡£: ä¸åŠ¨äº§æ± .md", file=sys.stderr)
        
        # ç”ŸæˆJSONå¤‡ä»½
        json_path = os.path.join(OUTPUT_DIR, 'data.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ç”ŸæˆJSONå¤‡ä»½: data.json", file=sys.stderr)
        
        print(f"\nğŸ‰ å¯¼å‡ºå®Œæˆï¼", file=sys.stderr)
        print(f"ğŸ“‚ æ–‡ä»¶ä½ç½®: {OUTPUT_DIR}", file=sys.stderr)
        print(f"ğŸ“„ ä¸»æ–‡æ¡£: ä¸åŠ¨äº§æ± .md", file=sys.stderr)
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60, file=sys.stderr)
    print("ğŸ˜ï¸  ä¸åŠ¨äº§æ± å­å¯¼å‡ºå·¥å…·", file=sys.stderr)
    print("=" * 60, file=sys.stderr)
    
    # è¯»å–æ•°æ®
    data = read_from_google_sheets()
    
    if not data:
        print("âŒ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º", file=sys.stderr)
        sys.exit(1)
    
    # å¯¼å‡ºæ•°æ®
    export_data(data)
    
    print("\nâœ¨ æ‰€æœ‰æ“ä½œå®Œæˆï¼", file=sys.stderr)


if __name__ == '__main__':
    main()

